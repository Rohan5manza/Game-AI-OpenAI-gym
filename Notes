Sources I used: https://blog.paperspace.com/getting-started-with-openai-gym/

Even though I am referring from existing projects done by others, I can do many things differently, to add uniqueness to my project, such as: 
1. If its an environment of Cartpole , i can penalize the agent for making large ovements with cart.
2. Use different RL algorithms, and track differences in performance of each.

Important note: RL is significantly different than traditional ML and neural networks that I am familiar with till now. Here, RL does not explicitly map 
relationships between state, rewards, and actions, whereas ML does. 
IN OTHER WORDS, RL INVOLVES THE AI AGENT LEARNING VIA EXPERIENCE, WHEREAS NEURAL- NETWORRK BASED APPROACHES RELY ON FUNCTION APPROXIMATION.

My notes: 

Env class is the fundamental building block of OpenAI gym. Python class which simulates the environment where you want to train the AI agent in.

Env has 2 main attributes: observation_space( which defines structure and legitimate values for 
observation of environment state), and action_space, which describes the numerical structure of 
legitimate actions that can be applied to the environment.

From the source code , from the article referred above:
We see that both the observation space as well as the action space are represented by classes called Box and Discrete, respectively. 
These are one of the various data structures provided by gym in order to implement observation and action spaces for different kind of scenarios (discrete action space, continuous action space, etc). 

In this section, we cover functions of the Env class that help the agent interact with the environment. Two such important functions are:

reset: This function resets the environment to its initial state, and returns the observation of the environment corresponding to the initial state.
step : This function takes an action as an input and applies it to the environment, which leads to the environment transitioning to a new state. The reset function returns four things:
observation: The observation of the state of the environment.
reward: The reward that you can get from the environment after executing the action that was given as the input to the step function.
done: Whether the episode has been terminated. If true, you may need to end the simulation or reset the environment to restart the episode.
info: This provides additional information depending on the environment, such as number of lives left, or general information that may be conducive in debugging.

In many cases, the observation is a screenshot of the game. 
Note that rendering images or animations in Google Colab notebook is extremely hard and prone to errors, I went through tedious process of installing dependencies
And it still didn't work. 
However, we are saved by the power of mathematics. No problem if we csn't see our Rl agent via animation , we can simply track the behaviour of the RL agent
via the values output by env.step()

In each time step, we track changes in the values printed by env.step(action)
let's take mountain car as example:
observation value: state of car, in form of array of values, consists of position and velocity of car.
reward: floating point number indicating reward received in current time step
done: boolean indicating whether episode is over or not
info: dictionary that keeps diagonistic information about the environment.


